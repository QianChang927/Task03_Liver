{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-27T13:41:56.627210Z",
     "start_time": "2025-06-27T13:41:52.079480Z"
    }
   },
   "source": [
    "from monai import transforms\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.data import CacheDataset, DataLoader, decollate_batch\n",
    "from monai.inferers import sliding_window_inference\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T13:41:56.649753Z",
     "start_time": "2025-06-27T13:41:56.639880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root_dir = '../Task_Dataset/Task03_Liver'\n",
    "val_percent = 0.1\n",
    "data_select = 20\n",
    "\n",
    "train_images = sorted(glob.glob(os.path.join(root_dir, 'imagesTr', '*.nii.gz')))\n",
    "train_labels = sorted(glob.glob(os.path.join(root_dir, 'labelsTr', '*.nii.gz')))\n",
    "test_images = sorted(glob.glob(os.path.join(root_dir, 'imagesTs', '*.nii.gz')))\n",
    "\n",
    "data_dicts = [{'image': image_name, 'label': label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
    "data_dicts = data_dicts[:data_select]\n",
    "\n",
    "num_percent = int(len(data_dicts) * val_percent)\n",
    "train_files, valid_files = data_dicts[:-num_percent], data_dicts[-num_percent:]\n",
    "print(f\"train length: {len(train_files)}, valid length: {len(valid_files)}\")"
   ],
   "id": "631799b6405aac8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length: 18, valid length: 2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T13:41:57.052207Z",
     "start_time": "2025-06-27T13:41:57.036853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "monai.utils.set_determinism(seed=0)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.LoadImaged(keys=['image', 'label']),\n",
    "    transforms.EnsureChannelFirstd(keys=['image', 'label']),\n",
    "    transforms.ScaleIntensityRanged(\n",
    "        keys=['image'],\n",
    "        a_min=-100,\n",
    "        a_max=200,\n",
    "        b_min=0.0,\n",
    "        b_max=1.0,\n",
    "        clip=True\n",
    "    ),\n",
    "    transforms.CropForegroundd(keys=['image', 'label'], source_key='image', allow_smaller=True),\n",
    "    transforms.Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "    transforms.Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.0), mode=('bilinear', 'nearest')),\n",
    "    transforms.RandCropByPosNegLabeld(\n",
    "        keys=['image', 'label'],\n",
    "        image_key='image',\n",
    "        label_key='label',\n",
    "        image_threshold=0,\n",
    "        spatial_size=(36, 36, 36),\n",
    "        pos=1,\n",
    "        neg=1,\n",
    "        num_samples=4\n",
    "    )\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.LoadImaged(keys=['image', 'label']),\n",
    "    transforms.EnsureChannelFirstd(keys=['image', 'label']),\n",
    "    transforms.ScaleIntensityRanged(\n",
    "        keys=['image'],\n",
    "        a_min=-100,\n",
    "        a_max=200,\n",
    "        b_min=0.0,\n",
    "        b_max=1.0,\n",
    "        clip=True\n",
    "    ),\n",
    "    transforms.CropForegroundd(keys=['image', 'label'], source_key='image', allow_smaller=True),\n",
    "    transforms.Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "    transforms.Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.0), mode=('bilinear', 'nearest'))\n",
    "])"
   ],
   "id": "4f22e134758da154",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T13:42:21.198517Z",
     "start_time": "2025-06-27T13:41:57.126896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 n_channels: list=None, batch_norm: bool=False) -> None:\n",
    "        super(UNet3D, self).__init__()\n",
    "        if n_channels is None:\n",
    "            n_channels = [64, 128, 256, 512]\n",
    "\n",
    "        self.in_conv = DoubleConv(in_channels, n_channels[0], batch_norm=batch_norm)\n",
    "        self.encoder_1 = DownSample(n_channels[0], n_channels[1], batch_norm=batch_norm)\n",
    "        self.encoder_2 = DownSample(n_channels[1], n_channels[2], batch_norm=batch_norm)\n",
    "        self.encoder_3 = DownSample(n_channels[2], n_channels[3], batch_norm=batch_norm)\n",
    "\n",
    "        self.decoder_1 = UpSample(n_channels[3], n_channels[2], n_channels[2], batch_norm=batch_norm)\n",
    "        self.decoder_2 = UpSample(n_channels[2], n_channels[1], n_channels[1], batch_norm=batch_norm)\n",
    "        self.decoder_3 = UpSample(n_channels[1], n_channels[0], n_channels[0], batch_norm=batch_norm)\n",
    "        self.out_conv = OutConv(n_channels[0], out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.in_conv(x)\n",
    "        x2 = self.encoder_1(x1)\n",
    "        x3 = self.encoder_2(x2)\n",
    "        x4 = self.encoder_3(x3)\n",
    "\n",
    "        x = self.decoder_1(x4, x3)\n",
    "        x = self.decoder_2(x, x2)\n",
    "        x = self.decoder_3(x, x1)\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, batch_norm: bool=False) -> None:\n",
    "        super(DoubleConv, self).__init__()\n",
    "        mid_channels = out_channels // 2\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        if batch_norm:\n",
    "            self.conv1.append(nn.BatchNorm3d(mid_channels))\n",
    "            self.conv2.append(nn.BatchNorm3d(out_channels))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, batch_norm: bool=False) -> None:\n",
    "        super(DownSample, self).__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            DoubleConv(in_channels, out_channels, batch_norm=batch_norm)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, encoder_channels: int, batch_norm: bool=False) -> None:\n",
    "        super(UpSample, self).__init__()\n",
    "        self.up = nn.ConvTranspose3d(in_channels, in_channels, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels + encoder_channels, out_channels, batch_norm=batch_norm)\n",
    "\n",
    "    def forward(self, decoder: torch.Tensor, encoder: torch.Tensor) -> torch.Tensor:\n",
    "        decoder = self.up(decoder)\n",
    "        x = torch.cat([encoder, decoder], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super(OutConv, self).__init__()\n",
    "        self.out = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.out(x)\n",
    "        return x"
   ],
   "id": "38a6293a81d3e9ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet3D(\n",
      "  (inputs): ConvDouble(\n",
      "    (conv_double): Sequential(\n",
      "      (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (down_1): DownSampling(\n",
      "    (downsample): Sequential(\n",
      "      (0): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): ConvDouble(\n",
      "        (conv_double): Sequential(\n",
      "          (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down_2): DownSampling(\n",
      "    (downsample): Sequential(\n",
      "      (0): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): ConvDouble(\n",
      "        (conv_double): Sequential(\n",
      "          (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down_3): DownSampling(\n",
      "    (downsample): Sequential(\n",
      "      (0): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): ConvDouble(\n",
      "        (conv_double): Sequential(\n",
      "          (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (4): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_1): UpSampling(\n",
      "    (up): ConvTranspose3d(512, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    (conv): ConvDouble(\n",
      "      (conv_double): Sequential(\n",
      "        (0): Conv3d(768, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_2): UpSampling(\n",
      "    (up): ConvTranspose3d(256, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    (conv): ConvDouble(\n",
      "      (conv_double): Sequential(\n",
      "        (0): Conv3d(384, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_3): UpSampling(\n",
      "    (up): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    (conv): ConvDouble(\n",
      "      (conv_double): Sequential(\n",
      "        (0): Conv3d(192, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output): Conv3d(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      ")\n",
      "torch.Size([8, 1, 96, 96, 96])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T13:44:13.852623Z",
     "start_time": "2025-06-27T13:42:21.460550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_cache = CacheDataset(data=train_files, transform=train_transform, cache_rate=1.0, num_workers=4)\n",
    "valid_cache = CacheDataset(data=valid_files, transform=valid_transform, cache_rate=1.0, num_workers=4)"
   ],
   "id": "952d79aaf31e282d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 18/18 [01:30<00:00,  5.02s/it]\n",
      "Loading dataset: 100%|██████████| 2/2 [00:22<00:00, 11.01s/it]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T13:44:14.089856Z",
     "start_time": "2025-06-27T13:44:14.080269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(dataset=train_cache, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_cache, batch_size=1, shuffle=False)"
   ],
   "id": "4b1add336f64ed38",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T13:44:14.643955Z",
     "start_time": "2025-06-27T13:44:14.351808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet3D(in_channels=1, out_channels=1, batch_norm=True).to(device)\n",
    "loss_fn = DiceLoss(to_onehot_y=True, softmax=True, squared_pred=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "dice_metric = DiceMetric(include_background=False, reduction='mean')\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n",
    "                                                       patience=2, threshold=1e-04, threshold_mode='rel',\n",
    "                                                       cooldown=0, min_lr=1e-06, eps=1e-08)"
   ],
   "id": "d566a991d513e868",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T13:44:14.837266Z",
     "start_time": "2025-06-27T13:44:14.830992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_EPOCHS = 100\n",
    "ROI_SIZE = (24, 24, 24)\n",
    "\n",
    "SW_BATCH_SIZE = 4\n",
    "VAL_INTERVAL = 2\n",
    "\n",
    "mean_dice = []\n",
    "train_loss = []\n",
    "\n",
    "best_dice = -1\n",
    "best_epoch = -1\n",
    "\n",
    "post_pred = transforms.Compose([transforms.AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = transforms.Compose([transforms.AsDiscrete(to_onehot=2)])"
   ],
   "id": "83bcfd1d7aa53fc8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T13:45:13.903215Z",
     "start_time": "2025-06-27T13:45:13.682303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(MAX_EPOCHS):\n",
    "    print(f\"{f'Epoch {epoch + 1}/{MAX_EPOCHS}':-^50}\")\n",
    "\n",
    "    train_step = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        train_step += 1\n",
    "        images, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{train_step}/{len(train_cache) // train_loader.batch_size}, train loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss /= train_step\n",
    "    train_loss.append(epoch_loss)\n",
    "    print(f\"epoch: {epoch + 1}/{MAX_EPOCHS}, average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if epoch % VAL_INTERVAL:\n",
    "        continue\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            images, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "            outputs = sliding_window_inference(images, ROI_SIZE, SW_BATCH_SIZE, model)\n",
    "            valid_outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
    "            valid_labels = [post_label(i) for i in decollate_batch(labels)]\n",
    "            dice_metric(y_pred=valid_outputs, y=valid_labels)\n",
    "\n",
    "        dice = dice_metric.aggregate().item()\n",
    "        scheduler.step(dice)\n",
    "\n",
    "        mean_dice.append(dice)\n",
    "        dice_metric.reset()\n",
    "\n",
    "        if dice > best_dice:\n",
    "            best_dice = dice\n",
    "            best_epoch = epoch + 1\n",
    "            # torch.save(model.state_dict(), f\"./results/best_dice_model.pth\")\n",
    "\n",
    "        print(f\"epoch: {epoch + 1}/{MAX_EPOCHS}, current mean dice: {dice:.4f}, \"\n",
    "              f\"best mean dice: {best_dice:.4f} at epoch {best_epoch}\")"
   ],
   "id": "1ce449800552efe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Epoch 1/100--------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 9 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     10\u001B[39m images, labels = batch[\u001B[33m'\u001B[39m\u001B[33mimage\u001B[39m\u001B[33m'\u001B[39m].to(device), batch[\u001B[33m'\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m'\u001B[39m].to(device)\n\u001B[32m     12\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m outputs = model(images)\n\u001B[32m     14\u001B[39m loss = loss_fn(outputs, labels)\n\u001B[32m     15\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\msd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\msd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 21\u001B[39m, in \u001B[36mUNet3D.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     18\u001B[39m x3 = \u001B[38;5;28mself\u001B[39m.down_2(x2)\n\u001B[32m     19\u001B[39m x4 = \u001B[38;5;28mself\u001B[39m.down_3(x3)\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m x5 = \u001B[38;5;28mself\u001B[39m.up_1(x4, x3)\n\u001B[32m     22\u001B[39m x6 = \u001B[38;5;28mself\u001B[39m.up_2(x5, x2)\n\u001B[32m     23\u001B[39m x7 = \u001B[38;5;28mself\u001B[39m.up_3(x6, x1)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\msd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\msd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 72\u001B[39m, in \u001B[36mUpSampling.forward\u001B[39m\u001B[34m(self, x1, x2)\u001B[39m\n\u001B[32m     70\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x1, x2):\n\u001B[32m     71\u001B[39m     x1 = \u001B[38;5;28mself\u001B[39m.up(x1)\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m     x = torch.cat([x2, x1], dim=\u001B[32m1\u001B[39m)\n\u001B[32m     73\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.conv(x)\n\u001B[32m     74\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\msd\\Lib\\site-packages\\monai\\data\\meta_tensor.py:283\u001B[39m, in \u001B[36mMetaTensor.__torch_function__\u001B[39m\u001B[34m(cls, func, types, args, kwargs)\u001B[39m\n\u001B[32m    281\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    282\u001B[39m     kwargs = {}\n\u001B[32m--> \u001B[39m\u001B[32m283\u001B[39m ret = \u001B[38;5;28msuper\u001B[39m().__torch_function__(func, types, args, kwargs)\n\u001B[32m    284\u001B[39m \u001B[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001B[39;00m\n\u001B[32m    285\u001B[39m \u001B[38;5;66;03m# if \"out\" in kwargs:\u001B[39;00m\n\u001B[32m    286\u001B[39m \u001B[38;5;66;03m#     return ret\u001B[39;00m\n\u001B[32m    287\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _not_requiring_metadata(ret):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\msd\\Lib\\site-packages\\torch\\_tensor.py:1648\u001B[39m, in \u001B[36mTensor.__torch_function__\u001B[39m\u001B[34m(cls, func, types, args, kwargs)\u001B[39m\n\u001B[32m   1645\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[32m   1647\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m _C.DisableTorchFunctionSubclass():\n\u001B[32m-> \u001B[39m\u001B[32m1648\u001B[39m     ret = func(*args, **kwargs)\n\u001B[32m   1649\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01min\u001B[39;00m get_default_nowrap_functions():\n\u001B[32m   1650\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[31mRuntimeError\u001B[39m: Sizes of tensors must match except in dimension 1. Expected size 9 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
